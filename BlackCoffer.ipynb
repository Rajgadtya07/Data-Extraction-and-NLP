{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjWoiIrFaHFH",
        "outputId": "dd217f4b-a148-438d-c111-ca131fad1233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframes in this file: Sheet1\n",
            "            URL_ID                                                URL\n",
            "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
            "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
            "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
            "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
            "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
            "Article blackassign0001 saved successfully.\n",
            "Article blackassign0002 saved successfully.\n",
            "Article blackassign0003 saved successfully.\n",
            "Article blackassign0004 saved successfully.\n",
            "Article blackassign0005 saved successfully.\n",
            "Article blackassign0006 saved successfully.\n",
            "Article blackassign0007 saved successfully.\n",
            "Article blackassign0008 saved successfully.\n",
            "Article blackassign0009 saved successfully.\n",
            "Article blackassign0010 saved successfully.\n",
            "Article blackassign0011 saved successfully.\n",
            "Article blackassign0012 saved successfully.\n",
            "Article blackassign0013 saved successfully.\n",
            "Article blackassign0014 saved successfully.\n",
            "Article blackassign0015 saved successfully.\n",
            "Article blackassign0016 saved successfully.\n",
            "Article blackassign0017 saved successfully.\n",
            "Article blackassign0018 saved successfully.\n",
            "Article blackassign0019 saved successfully.\n",
            "Article blackassign0020 saved successfully.\n",
            "Article blackassign0021 saved successfully.\n",
            "Article blackassign0022 saved successfully.\n",
            "Article blackassign0023 saved successfully.\n",
            "Article blackassign0024 saved successfully.\n",
            "Article blackassign0025 saved successfully.\n",
            "Article blackassign0026 saved successfully.\n",
            "Article blackassign0027 saved successfully.\n",
            "Article blackassign0028 saved successfully.\n",
            "Article blackassign0029 saved successfully.\n",
            "Article blackassign0030 saved successfully.\n",
            "Article blackassign0031 saved successfully.\n",
            "Article blackassign0032 saved successfully.\n",
            "Article blackassign0033 saved successfully.\n",
            "Article blackassign0034 saved successfully.\n",
            "Article blackassign0035 saved successfully.\n",
            "Error extracting article blackassign0036: 'NoneType' object has no attribute 'get_text'\n",
            "Article blackassign0037 saved successfully.\n",
            "Article blackassign0038 saved successfully.\n",
            "Article blackassign0039 saved successfully.\n",
            "Article blackassign0040 saved successfully.\n",
            "Article blackassign0041 saved successfully.\n",
            "Article blackassign0042 saved successfully.\n",
            "Article blackassign0043 saved successfully.\n",
            "Article blackassign0044 saved successfully.\n",
            "Article blackassign0045 saved successfully.\n",
            "Article blackassign0046 saved successfully.\n",
            "Article blackassign0047 saved successfully.\n",
            "Article blackassign0048 saved successfully.\n",
            "Error extracting article blackassign0049: 'NoneType' object has no attribute 'get_text'\n",
            "Article blackassign0050 saved successfully.\n",
            "Article blackassign0051 saved successfully.\n",
            "Article blackassign0052 saved successfully.\n",
            "Article blackassign0053 saved successfully.\n",
            "Article blackassign0054 saved successfully.\n",
            "Article blackassign0055 saved successfully.\n",
            "Article blackassign0056 saved successfully.\n",
            "Article blackassign0057 saved successfully.\n",
            "Article blackassign0058 saved successfully.\n",
            "Article blackassign0059 saved successfully.\n",
            "Article blackassign0060 saved successfully.\n",
            "Article blackassign0061 saved successfully.\n",
            "Article blackassign0062 saved successfully.\n",
            "Article blackassign0063 saved successfully.\n",
            "Article blackassign0064 saved successfully.\n",
            "Article blackassign0065 saved successfully.\n",
            "Article blackassign0066 saved successfully.\n",
            "Article blackassign0067 saved successfully.\n",
            "Article blackassign0068 saved successfully.\n",
            "Article blackassign0069 saved successfully.\n",
            "Article blackassign0070 saved successfully.\n",
            "Article blackassign0071 saved successfully.\n",
            "Article blackassign0072 saved successfully.\n",
            "Article blackassign0073 saved successfully.\n",
            "Article blackassign0074 saved successfully.\n",
            "Article blackassign0075 saved successfully.\n",
            "Article blackassign0076 saved successfully.\n",
            "Article blackassign0077 saved successfully.\n",
            "Article blackassign0078 saved successfully.\n",
            "Article blackassign0079 saved successfully.\n",
            "Article blackassign0080 saved successfully.\n",
            "Article blackassign0081 saved successfully.\n",
            "Article blackassign0082 saved successfully.\n",
            "Article blackassign0083 saved successfully.\n",
            "Article blackassign0084 saved successfully.\n",
            "Article blackassign0085 saved successfully.\n",
            "Article blackassign0086 saved successfully.\n",
            "Article blackassign0087 saved successfully.\n",
            "Article blackassign0088 saved successfully.\n",
            "Article blackassign0089 saved successfully.\n",
            "Article blackassign0090 saved successfully.\n",
            "Article blackassign0091 saved successfully.\n",
            "Article blackassign0092 saved successfully.\n",
            "Article blackassign0093 saved successfully.\n",
            "Article blackassign0094 saved successfully.\n",
            "Article blackassign0095 saved successfully.\n",
            "Article blackassign0096 saved successfully.\n",
            "Article blackassign0097 saved successfully.\n",
            "Article blackassign0098 saved successfully.\n",
            "Article blackassign0099 saved successfully.\n",
            "Article blackassign0100 saved successfully.\n",
            "Final dataset prepared and saved as 'final_dataset.xlsx'.\n"
          ]
        }
      ],
      "source": [
        "!pip install xlrd\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install textblob\n",
        "!pip install nltk\n",
        "!pip install textstat\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "import textstat\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# NLTK downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "def read_all_sheets_from_excel(excel_file):\n",
        "    \"\"\"\n",
        "    Reads all sheets from an Excel file and returns a dictionary of dataframes.\n",
        "\n",
        "    Args:\n",
        "        excel_file (str): Path to the Excel file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where the keys are the sheet names and the values are the corresponding dataframes.\n",
        "    \"\"\"\n",
        "    xl = pd.ExcelFile(excel_file)\n",
        "    dataframes = {sheet_name: xl.parse(sheet_name) for sheet_name in xl.sheet_names}\n",
        "    return dataframes\n",
        "\n",
        "dataframes = read_all_sheets_from_excel('Input.xlsx')\n",
        "\n",
        "# print each dataframe name\n",
        "print(\"Dataframes in this file:\", \", \".join(dataframes.keys()))\n",
        "\n",
        "for k, v in dataframes.items():\n",
        "    print(v.head())\n",
        "\n",
        "# Function to extract and save article text\n",
        "def extract_and_save_article(url, url_id):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Assuming the article title is within <h1> tags and the text within <p> tags\n",
        "        # This may need adjustment based on the actual structure of the target webpages\n",
        "        article_title = soup.find('h1').get_text()\n",
        "        article_text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
        "\n",
        "        # Combine title and text\n",
        "        full_text = article_title + '\\n\\n' + article_text\n",
        "\n",
        "        # Save to a text file named after the URL_ID\n",
        "        with open(f'{url_id}.txt', 'w', encoding='utf-8') as file:\n",
        "            file.write(full_text)\n",
        "\n",
        "        print(f'Article {url_id} saved successfully.')\n",
        "    except Exception as e:\n",
        "        print(f'Error extracting article {url_id}: {e}')\n",
        "\n",
        "# Loop through each URL in the DataFrame\n",
        "for index, row in dataframes['Sheet1'].iterrows():\n",
        "    extract_and_save_article(row['URL'], row['URL_ID'])\n",
        "\n",
        "# Function to compute scores\n",
        "def compute_scores(text):\n",
        "    # Use TextBlob for subjectivity and polarity\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "    # Initialize Sentiment Intensity Analyzer\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Use NLTK VADER for positive and negative scores\n",
        "    sentiment_scores = sid.polarity_scores(text)\n",
        "    positive_score = sentiment_scores['pos']\n",
        "    negative_score = sentiment_scores['neg']\n",
        "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score)\n",
        "    subjectivity_score = (positive_score + negative_score) / len(blob.words)\n",
        "    avg_sentence_length = len(blob.words) / len(blob.sentences)\n",
        "    complex_words = sum(1 for word in blob.words if textstat.syllable_count(word) >= 3)\n",
        "    percentage_complex_words = complex_words / len(blob.words) * 100\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "    avg_words_per_sentence = avg_sentence_length  # This is the same as avg_sentence_length\n",
        "    word_count = len(blob.words)\n",
        "    syllable_per_word = sum(textstat.syllable_count(word) for word in blob.words) / len(blob.words)\n",
        "    personal_pronouns = sum(1 for word in blob.words if word.lower() in ['i', 'we', 'you', 'he', 'she', 'they'])\n",
        "    avg_word_length = sum(len(word) for word in blob.words) / len(blob.words)\n",
        "\n",
        "    return positive_score, negative_score, polarity, subjectivity, avg_sentence_length, percentage_complex_words, \\\n",
        "           fog_index, avg_words_per_sentence, word_count, syllable_per_word, personal_pronouns, avg_word_length\n",
        "\n",
        "# Assuming article texts are saved in the current directory with their URL_ID as filenames\n",
        "article_files = [f for f in os.listdir('.') if f.endswith('.txt')]\n",
        "\n",
        "# Prepare the final dataset\n",
        "data = []\n",
        "for file in article_files:\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        pos_score, neg_score, polarity, subjectivity, avg_sentence_length, percentage_complex_words, fog_index, \\\n",
        "        avg_words_per_sentence, word_count, syllable_per_word, personal_pronouns, avg_word_length = compute_scores(\n",
        "            text)\n",
        "        data.append({\n",
        "            'URL_ID': file.replace('.txt', ''),\n",
        "            'Positive Score': pos_score,\n",
        "            'Negative Score': neg_score,\n",
        "            'Polarity Score': polarity,\n",
        "            'Subjectivity Score': subjectivity,\n",
        "            'Average Sentence Length': avg_sentence_length,\n",
        "            'Percentage of Complex Words': percentage_complex_words,\n",
        "            'Fog Index': fog_index,\n",
        "            'Word Count': word_count,\n",
        "            'Syllable per Word': syllable_per_word,\n",
        "            'Personal Pronouns': personal_pronouns,\n",
        "            'Average Word Length': avg_word_length\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame\n",
        "final_df = pd.DataFrame(data)\n",
        "\n",
        "# Optionally, merge with the original DataFrame to include URLs\n",
        "# original_df = pd.read_excel('Input.xlsx')\n",
        "# final_df = pd.merge(original_df, final_df, on='URL_ID')\n",
        "\n",
        "# Save the final dataset to a new Excel file\n",
        "final_df.to_excel('final_dataset.xlsx', index=False)\n",
        "\n",
        "print(\"Final dataset prepared and saved as 'final_dataset.xlsx'.\")\n"
      ]
    }
  ]
}